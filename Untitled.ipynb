{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIIT Delhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A jsonlines file called preds.jsonl which contains your model's predictions on the sentihood test set. Please ensure that the original sample, and its annotated aspects/sentiments are also included. The model should take the text and the target entity as inputs and predict the aspect as well as the sentiment for each predicted sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import json  \n",
    "import pandas as pd  \n",
    "from pandas.io.json import json_normalize \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>opinions</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430</td>\n",
       "      <td>[{'sentiment': 'Negative', 'aspect': 'price', ...</td>\n",
       "      <td>LOCATION1 is transforming and the prices w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>[{'sentiment': 'Positive', 'aspect': 'shopping...</td>\n",
       "      <td>Along LOCATION1 there are lots of Electronic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244</td>\n",
       "      <td>[{'sentiment': 'Positive', 'aspect': 'transit-...</td>\n",
       "      <td>And LOCATION1 is ten mins direct on the tube...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>209</td>\n",
       "      <td>[{'sentiment': 'Positive', 'aspect': 'nightlif...</td>\n",
       "      <td>Another option is LOCATION1 which is very ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2824</td>\n",
       "      <td>[{'sentiment': 'Positive', 'aspect': 'general'...</td>\n",
       "      <td>Best bet is around LOCATION2 and LOCATION1 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1835</td>\n",
       "      <td>[{'sentiment': 'Negative', 'aspect': 'transit-...</td>\n",
       "      <td>Central London based taxis mostly refuse far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1429</td>\n",
       "      <td>[{'sentiment': 'Negative', 'aspect': 'general'...</td>\n",
       "      <td>Don't go looking at places like LOCATION1  n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1404</td>\n",
       "      <td>[]</td>\n",
       "      <td>Down here in South London the accent [local]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>190</td>\n",
       "      <td>[{'sentiment': 'Positive', 'aspect': 'multicul...</td>\n",
       "      <td>Everyone in LOCATION1 is now black or Bangla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1281</td>\n",
       "      <td>[{'sentiment': 'Negative', 'aspect': 'general'...</td>\n",
       "      <td>For gods sake don't move to LOCATION1 its ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           opinions  \\\n",
       "0  1430  [{'sentiment': 'Negative', 'aspect': 'price', ...   \n",
       "1  2013  [{'sentiment': 'Positive', 'aspect': 'shopping...   \n",
       "2  1244  [{'sentiment': 'Positive', 'aspect': 'transit-...   \n",
       "3   209  [{'sentiment': 'Positive', 'aspect': 'nightlif...   \n",
       "4  2824  [{'sentiment': 'Positive', 'aspect': 'general'...   \n",
       "5  1835  [{'sentiment': 'Negative', 'aspect': 'transit-...   \n",
       "6  1429  [{'sentiment': 'Negative', 'aspect': 'general'...   \n",
       "7  1404                                                 []   \n",
       "8   190  [{'sentiment': 'Positive', 'aspect': 'multicul...   \n",
       "9  1281  [{'sentiment': 'Negative', 'aspect': 'general'...   \n",
       "\n",
       "                                                text  \n",
       "0      LOCATION1 is transforming and the prices w...  \n",
       "1    Along LOCATION1 there are lots of Electronic...  \n",
       "2    And LOCATION1 is ten mins direct on the tube...  \n",
       "3    Another option is LOCATION1 which is very ce...  \n",
       "4    Best bet is around LOCATION2 and LOCATION1 a...  \n",
       "5    Central London based taxis mostly refuse far...  \n",
       "6    Don't go looking at places like LOCATION1  n...  \n",
       "7    Down here in South London the accent [local]...  \n",
       "8    Everyone in LOCATION1 is now black or Bangla...  \n",
       "9    For gods sake don't move to LOCATION1 its ho...  "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Data/sentihood-train.json', mode = 'r') as f: \n",
    "    d = json.load(f)    \n",
    "\n",
    "data = json_normalize(d)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "aspect = []\n",
    "target_entity = []\n",
    "for dic in data[['opinions']][:].items():\n",
    "    \n",
    "    for t in dic[1]:\n",
    "        if t == []:\n",
    "            t.append({'sentiment': '', 'aspect': '', 'target_entity': ''})\n",
    "            \n",
    "        for a in t:\n",
    "            sentiment.append(a.get('sentiment'))\n",
    "            aspect.append(a.get('aspect'))\n",
    "            target_entity.append(a.get('target_entity'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_item(list1): \n",
    "    a = np.array(list1) \n",
    "    print(np.unique(a)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'general': 1182, '': 956, 'price': 500, 'transit-location': 431, 'safety': 352, 'live': 221, 'nightlife': 158, 'shopping': 143, 'multicultural': 123, 'green-nature': 95, 'dining': 93, 'quiet': 54, 'touristy': 49})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(aspect)\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Sentiments: \n",
      "['' 'Negative' 'Positive']\n",
      "Unique Aspect: \n",
      "['' 'dining' 'general' 'green-nature' 'live' 'multicultural' 'nightlife'\n",
      " 'price' 'quiet' 'safety' 'shopping' 'touristy' 'transit-location']\n",
      "Unique Target Entity: \n",
      "['' 'LOCATION1' 'LOCATION2']\n"
     ]
    }
   ],
   "source": [
    "print('Unique Sentiments: ')\n",
    "unique_item(sentiment)\n",
    "print('Unique Aspect: ')\n",
    "unique_item(aspect)\n",
    "print('Unique Target Entity: ')\n",
    "unique_item(target_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 occurences other than None\n",
    "aspect_to_id = {\n",
    "    'general' : 0,\n",
    "    'price': 1,\n",
    "    'transit-location' : 2,\n",
    "    'safety' : 3, \n",
    "    'live' : 4, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(file):\n",
    "    with open(file) as f:\n",
    "        d = json.load(f)\n",
    "        \n",
    "#     d = json_normalize(d)\n",
    "#     _df = list()\n",
    "\n",
    "    _df = list()\n",
    "    \n",
    "    for i in d:\n",
    "        _test = i['text']\n",
    "        _id = i['id']\n",
    "        opinions = list()\n",
    "        \n",
    "        for j in i['opinions']:\n",
    "            \n",
    "            _sent = j['sentiment']\n",
    "            _aspe = j['aspect']\n",
    "            _tare = j['target_entity']\n",
    "            \n",
    "#             for k in j:            \n",
    "#                 _sent = j['sentiment']\n",
    "#                 _aspe = j['aspect']\n",
    "#                 _tare = j['target_entity']\n",
    "            \n",
    "            opinions.append((_tare, _aspe, _sent))\n",
    "        _df.append((_id, _test, opinions))\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, aspectid):\n",
    "    \n",
    "    preprocessed_data_1 = list()\n",
    "    \n",
    "    for _id, _txt, _op in data:\n",
    "        for _tare, _aspe, _sent in _op:\n",
    "            if _aspe not in aspectid:\n",
    "                continue\n",
    "            preprocessed_data_1.append((_id, _txt, _tare, _aspe, _sent))\n",
    "\n",
    "            assert 'LOCATION1' in _txt\n",
    "            _tars = set(['LOCATION1'])\n",
    "        \n",
    "            if 'LOCATION2' in _txt:\n",
    "                _tars.add('LOCATION2')\n",
    "                \n",
    "            for tar in _tars:\n",
    "                \n",
    "                aspects = set([a for t, a, _ in _op if t == tar])\n",
    "                none_aspects = [a for a in aspectid if a not in aspects]\n",
    "                \n",
    "                for aspect in none_aspects:\n",
    "                    preprocessed_data_1.append((_id, _txt, tar, aspect, 'None'))\n",
    "    \n",
    "    \n",
    "    # Extracting the aspect for ABSA\n",
    "    preprocessed_data_2 = list()\n",
    "    \n",
    "    for _, _, _, aspect, _ in preprocessed_data_1:\n",
    "        preprocessed_data_2.append(aspectid[aspect])\n",
    "        \n",
    "        \n",
    "    assert len(preprocessed_data_1) == len(preprocessed_data_2)\n",
    "    \n",
    "    preprocessed_data_2 = np.array(preprocessed_data_2)\n",
    "\n",
    "    \n",
    "    # Creating tokens \n",
    "    preprocessed_data_final = list()\n",
    "                                               \n",
    "#     print(len(preprocessed_data_2))\n",
    "    \n",
    "    for _id, _txt, _tare, _aspe, _sent in preprocessed_data_1:\n",
    "        \n",
    "        _txt_1 = nltk.word_tokenize(_txt)\n",
    "        _aspe_1 = _aspe.split('-')\n",
    "        \n",
    "        preprocessed_data_final.append((_id, _txt_1, _tare, _aspe_1, _sent))\n",
    "    \n",
    "    return preprocessed_data_2, preprocessed_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = parse_json('./Data/sentihood-train.json')\n",
    "val = parse_json('./Data/sentihood-dev.json')\n",
    "test = parse_json('./Data/sentihood-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2977"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length =  2977\n",
      "val length =  747\n",
      "test length =  1491\n"
     ]
    }
   ],
   "source": [
    "print(\"train length = \", len(train))\n",
    "print(\"val length = \", len(dev))\n",
    "print(\"test length = \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id, train = preprocess_data(train, aspect_to_id)\n",
    "val_id, val = preprocess_data(val, aspect_to_id)\n",
    "test_id, test = preprocess_data(test, aspect_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length =  16226\n",
      "val length =  747\n",
      "test length =  8052\n"
     ]
    }
   ],
   "source": [
    "print(\"train length = \", len(train))\n",
    "print(\"val length = \", len(dev))\n",
    "print(\"test length = \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort(key=lambda x:x[2]+str(x[0])+x[3][0])\n",
    "val.sort(key=lambda y:y[2]+str(y[0])+y[3][0])\n",
    "test.sort(key=lambda z:z[2]+str(z[0])+z[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_dir+'bert-pair/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which aspects and their respective sentiments does your model most accurately detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What the points of failure for the model - which aspects does it perform poorly on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick paragraph in a 100 words or less about your favourite machine learning library and what you dislike about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
